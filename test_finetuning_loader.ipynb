{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch_em.data import MinInstanceSampler\n",
    "import micro_sam.training as sam_training\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from torch_em.util.debug import check_loader\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.ndimage import binary_erosion\n",
    "import numpy as np\n",
    "from skimage.measure import label\n",
    "from skimage.io import imread\n",
    "import torch_em\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# create a torch dataset\n",
    "# create a torch dataset\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir,\n",
    "        mask_dir,\n",
    "        crop_size=None,\n",
    "        padding_size=8,\n",
    "        with_segmentation_decoder=True,\n",
    "        augmentation=False):\n",
    "\n",
    "      self.images = list(sorted(glob(image_dir)))\n",
    "      self.masks = list(sorted(glob(mask_dir)))\n",
    "\n",
    "      self.crop_size = crop_size\n",
    "      self.padding_size = padding_size\n",
    "      self.with_segmentation_decoder = with_segmentation_decoder\n",
    "      self.augmentation = augmentation\n",
    "\n",
    "      if len(self.images) == 0 or len(self.images) != len(self.masks):\n",
    "          raise Exception('something wrong with the directory')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.images)\n",
    "\n",
    "    # function to erode label boundaries\n",
    "    def erode(self, labels, iterations, border_value):\n",
    "\n",
    "      foreground = np.zeros_like(labels, dtype=bool)\n",
    "\n",
    "      # loop through unique labels\n",
    "      for label in np.unique(labels):\n",
    "\n",
    "          # skip background\n",
    "          if label == 0:\n",
    "              continue\n",
    "\n",
    "          # mask to label\n",
    "          label_mask = labels == label\n",
    "\n",
    "          # erode labels\n",
    "          eroded_mask = binary_erosion(\n",
    "                  label_mask,\n",
    "                  iterations=iterations,\n",
    "                  border_value=border_value)\n",
    "\n",
    "          # get foreground\n",
    "          foreground = np.logical_or(eroded_mask, foreground)\n",
    "\n",
    "      # and background...\n",
    "      background = np.logical_not(foreground)\n",
    "\n",
    "      # set eroded pixels to zero\n",
    "      labels[background] = 0\n",
    "\n",
    "      return labels\n",
    "\n",
    "    # takes care of padding\n",
    "    def get_padding(self, crop_size, padding_size):\n",
    "    \n",
    "        # quotient\n",
    "        q = int(crop_size / padding_size)\n",
    "    \n",
    "        if crop_size % padding_size != 0:\n",
    "            padding = (padding_size * (q + 1))\n",
    "        else:\n",
    "            padding = crop_size\n",
    "    \n",
    "        return padding\n",
    "    \n",
    "    # sample augmentations (see https://albumentations.ai/docs/examples/example_kaggle_salt)\n",
    "    def augment_data(self, raw, mask, padding):\n",
    "          \n",
    "        transform = A.Compose([\n",
    "              A.RandomCrop(\n",
    "                  width=self.crop_size,\n",
    "                  height=self.crop_size),\n",
    "              A.PadIfNeeded(\n",
    "                  min_height=padding,\n",
    "                  min_width=padding,\n",
    "                  p=1,\n",
    "                  border_mode=0),\n",
    "              A.HorizontalFlip(p=0.3),\n",
    "              A.VerticalFlip(p=0.3),\n",
    "              A.RandomRotate90(p=0.3),\n",
    "              A.Transpose(p=0.3),\n",
    "              A.RandomBrightnessContrast(p=0.3)\n",
    "            ])\n",
    "\n",
    "        transformed = transform(image=raw, mask=mask)\n",
    "\n",
    "        raw, mask = transformed['image'], transformed['mask']\n",
    "\n",
    "        # I guess that the training and loader deals with 3d pictures, so it wants another axis\n",
    "        if len(raw.shape) == 2:\n",
    "            raw = np.expand_dims(raw, axis=0)\n",
    "            #mask = np.expand_dims(mask, axis=0)\n",
    "        return raw, mask\n",
    "\n",
    "    # normalize raw data between 0 and 1\n",
    "    def normalize(self, data):\n",
    "      return ((data - np.min(data)) / (np.max(data) - np.min(data))*255).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "      raw = self.images[idx]\n",
    "      labels = self.masks[idx]\n",
    "\n",
    "      raw = imread(raw)\n",
    "      raw = self.normalize(raw)\n",
    "      \n",
    "      # slice first channel, relabel connected components\n",
    "      labels = label(imread(labels)).astype(np.uint16)\n",
    "\n",
    "      padding = self.get_padding(self.crop_size, self.padding_size)\n",
    "      if self.augmentation:\n",
    "        raw, labels = self.augment_data(raw, labels, padding)\n",
    "\n",
    "      min_size = 25\n",
    "      if self.with_segmentation_decoder:\n",
    "        label_transform = torch_em.transform.label.PerObjectDistanceTransform(\n",
    "            distances=True,\n",
    "            boundary_distances=True,\n",
    "            directed_distances=False,\n",
    "            foreground=True,\n",
    "            instances=True,\n",
    "            min_size=min_size,\n",
    "        )\n",
    "      else:\n",
    "        label_transform = torch_em.transform.label.MinSizeLabelTransform(min_size=min_size)\n",
    "      labels = label_transform(labels).astype(np.float32)\n",
    "\n",
    "      return raw, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = ('GFP_max', 'GFP_max_clahe', 'GFP_sum')\n",
    "seg_ds = ('CELL_max', 'CELL_comb', 'CELL_unique', 'CELL_manual')\n",
    "\n",
    "models = ('vit_t_lm', 'vit_b_lm', 'vit_l_lm')\n",
    "\n",
    "\n",
    "root_dir = '/group/jug/Enrico/TISSUE_roi_projection'\n",
    "\n",
    "raw_dataset = raw_ds[0]\n",
    "segmentation_gt = seg_ds[0]\n",
    "model = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from multiple files in folder via pattern (here: all tif files)\n",
    "raw_key = '*_' + raw_dataset + '.tif'\n",
    "label_key = '*_' + segmentation_gt + '.tif'\n",
    "\n",
    "# already splitted into two folders\n",
    "training_dir = os.path.join(root_dir, \"training\")\n",
    "validation_dir = os.path.join(root_dir, \"validation\")\n",
    "\n",
    "# Here, we use `micro_sam.training.default_sam_loader` for creating a suitable data loader from\n",
    "# the example hela data. You can either adapt this for your own data or write a suitable torch dataloader yourself.\n",
    "# Here's a quickstart notebook to create your own dataloaders: https://github.com/constantinpape/torch-em/blob/main/notebooks/tutorial_create_dataloaders.ipynb\n",
    "\n",
    "# Train an additional convolutional decoder for end-to-end automatic instance segmentation\n",
    "# NOTE 1: It's important to have densely annotated-labels while training the additional convolutional decoder.\n",
    "# NOTE 2: In case you do not have labeled images, we recommend using `micro-sam` annotator tools to annotate as many objects as possible per image for best performance.\n",
    "train_instance_segmentation = True\n",
    "\n",
    "# NOTE: The dataloader internally takes care of adding label transforms: i.e. used to convert the ground-truth\n",
    "# labels to the desired instances for finetuning Segment Anythhing, or, to learn the foreground and distances\n",
    "# to the object centers and object boundaries for automatic segmentation.\n",
    "\n",
    "# There are cases where our inputs are large and the labeled objects are not evenly distributed across the image.\n",
    "# For this we use samplers, which ensure that valid inputs are chosen subjected to the paired labels.\n",
    "# The sampler chosen below makes sure that the chosen inputs have atleast one foreground instance, and filters out small objects.\n",
    "sampler = MinInstanceSampler(min_size=25)  # NOTE: The choice of 'min_size' value is paired with the same value in 'min_size' filter in 'label_transform'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_objects_per_batch = 1  # the number of objects per batch that will be sampled\n",
    "crop_size = 800  # the size of patches\n",
    "\n",
    "# train dataset and loader, with augmentation\n",
    "train_dataset = CellDataset(\n",
    "    image_dir = os.path.join(training_dir, raw_key),\n",
    "    mask_dir = os.path.join(training_dir, label_key),\n",
    "    crop_size=crop_size, with_segmentation_decoder=True, augmentation=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=n_objects_per_batch, shuffle=True, drop_last=True)\n",
    "train_loader.shuffle = True\n",
    "\n",
    "# validation dataset and loadet, no augmentation\n",
    "valid_dataset = CellDataset(\n",
    "    image_dir = os.path.join(validation_dir, raw_key),\n",
    "    mask_dir = os.path.join(validation_dir, label_key),\n",
    "    crop_size=crop_size, with_segmentation_decoder=True, augmentation=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=n_objects_per_batch, shuffle=True, drop_last=True)\n",
    "valid_loader.shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n",
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n",
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n",
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n",
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying labels in 'train' dataloader: 100%|██████████| 10/10 [00:02<00:00,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n",
      "raw shape: (1, 800, 800)   labels shape: (4, 800, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# All hyperparameters for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # the device/GPU used for training\n",
    "n_epochs = 5  # how long we train (in epochs)\n",
    "\n",
    "# The model_type determines which base model is used to initialize the weights that are finetuned.\n",
    "# We use vit_b here because it can be trained faster. Note that vit_h usually yields higher quality results.\n",
    "model_type = model\n",
    "\n",
    "# The name of the checkpoint. The checkpoints will be stored in './checkpoints/<checkpoint_name>'\n",
    "checkpoint_name = model + '---' + raw_dataset + '---' + segmentation_gt\n",
    "\n",
    "output_folder = 'test_models'\n",
    "\n",
    "if not os.path.exists(os.path.join(root_dir, output_folder)):\n",
    "    os.makedirs(os.path.join(root_dir, output_folder))\n",
    "\n",
    "sam_training.train_sam(\n",
    "    name=checkpoint_name,\n",
    "    save_root=os.path.join(root_dir, output_folder),\n",
    "    model_type=model_type,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    n_epochs=n_epochs,\n",
    "    n_objects_per_batch=n_objects_per_batch,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    verify_n_labels_in_loader=10,\n",
    "    device=device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
